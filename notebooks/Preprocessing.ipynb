{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies"
      ],
      "metadata": {
        "id": "mVeX0-ar_oel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas"
      ],
      "metadata": {
        "id": "GSVY5wlg_hSI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0eeae9-e29f-4dc0-9b73-37157695400c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C02RL8FrX5Sm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/reddit.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "f7HwZ7EdEVQ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "47eafa2d-a0f7-4981-dbff-37f3c99df6df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/reddit.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4130050935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/reddit.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/reddit.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "qcsPbdjeEwaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Viewing some samples from the clean_comment column"
      ],
      "metadata": {
        "id": "LpgVnuPrAGTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample()['clean_comment'].values"
      ],
      "metadata": {
        "id": "SCxVjZS6E8_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "6cEmDt42FGB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "rK1JHF1BFg7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing the null values**\n",
        "> If the number of missing values are reasonable to the whole dataset, only then we should be dropping the missing columns\n",
        "\n"
      ],
      "metadata": {
        "id": "I35m9TpHAQHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['clean_comment'].isna()]"
      ],
      "metadata": {
        "id": "fBhv-L5uFnhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['clean_comment'].isna()]['category'].value_counts()"
      ],
      "metadata": {
        "id": "aJLNxeO8F1Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "stnQIrjpGQRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for the duplicate values**"
      ],
      "metadata": {
        "id": "1dJ_9pxHAuSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "071G_WzNGjCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated()]"
      ],
      "metadata": {
        "id": "l1okXPLFGk4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping the duplicate values as the number of duplicates is only 350 to 37,000"
      ],
      "metadata": {
        "id": "a3F-R9fGAzy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "aAIkWlQEGvrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "vOrkEBKIGxdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for New Lines** \\\n",
        "Sometimes these new lines can be considered as a row itself."
      ],
      "metadata": {
        "id": "gJ6C_3CfBJ4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df['clean_comment'].str.strip() == '')]"
      ],
      "metadata": {
        "id": "Ns6WOfSOG1om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= df[~(df['clean_comment'].str.strip() == '')]"
      ],
      "metadata": {
        "id": "ALoNQSffM3Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lowercasing all the comments**"
      ],
      "metadata": {
        "id": "jl1ie03gBfl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the 'clean_comment' column to lowercase\n",
        "df['clean_comment'] = df['clean_comment'].str.lower()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "imwFw5uLNHwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing the whitespaces before and after the comments** \\\n",
        "In NLP context, the presence of whitespaces as a prefix or suffix can cost extra burn of tokens, so they are considered to be removed for efficiency."
      ],
      "metadata": {
        "id": "yfpbLgP0Bk_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['clean_comment'].apply(lambda x: x.endswith(' ') or x.startswith(' '))]"
      ],
      "metadata": {
        "id": "kKc2Hxq0NYYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove trailing and leading whitespaces from the 'clean_column' column\n",
        "df['clean_comment'] = df['clean_comment'].str.strip()\n",
        "\n",
        "# Verify the transformation by checking for any remaining trailing whitespaces\n",
        "df[df['clean_comment'].apply(lambda x: x.endswith(' ') or x.startswith(' '))]"
      ],
      "metadata": {
        "id": "sbZdy8WCNrIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for URLs in the data**"
      ],
      "metadata": {
        "id": "X8P0yvCRCP7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify comments containing URLs\n",
        "url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "comments_with_urls= df[df['clean_comment'].str.contains(url_pattern, regex=True)]\n",
        "\n",
        "# Display\n",
        "comments_with_urls.head()"
      ],
      "metadata": {
        "id": "eqpco7o_OGNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing new line characters (\\n) from the the comments column**"
      ],
      "metadata": {
        "id": "dogrAV1aCYgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify comments containing new line characters\n",
        "comments_with_newline = df[df['clean_comment'].str.contains('\\n')]\n",
        "\n",
        "# Display\n",
        "comments_with_newline.head()"
      ],
      "metadata": {
        "id": "dm7yPH6-PyBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove new line characters from the 'clean_comment' column\n",
        "df['clean_comment'] = df['clean_comment'].str.replace('\\n', ' ', regex=True)\n",
        "\n",
        "# Verify\n",
        "comments_with_newline_remaining = df[df['clean_comment'].str.contains('\\n')]\n",
        "comments_with_newline_remaining"
      ],
      "metadata": {
        "id": "n1O4ce9wQGCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EDA (Exploratory Data Analysis)**\n",
        "In this section we will visualize the data in various point of views to get the information and insights out of it. EDA is performed to uncover hidden patterns, spot anomalies or outliers, and test underlying assumptions before applying formal modeling or machine learning techniques."
      ],
      "metadata": {
        "id": "LY5dT3vHQjUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the visualization libraries**"
      ],
      "metadata": {
        "id": "qWm5ACrcDIUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn matplotlib.pyplot"
      ],
      "metadata": {
        "id": "SY-LiHH7DNnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of the classes\n",
        "# positive(1) negative(-1) and neutral(0) categories\n",
        "\n",
        "sns.countplot(data=df, x=\"category\")"
      ],
      "metadata": {
        "id": "L3EKHJZNQhd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frequency distribution of sentiments\n",
        "df['category'].value_counts(normalize=True).mul(100).round(2)"
      ],
      "metadata": {
        "id": "h-svAMJmRynF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding a new column which consists the number of words in a comment"
      ],
      "metadata": {
        "id": "bo-Z5H83Dzic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'] = df['clean_comment'].apply(lambda x: len(x.split()))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OcZQku-QR8yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['word_count'].describe()"
      ],
      "metadata": {
        "id": "NSylUURlSEun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the word count"
      ],
      "metadata": {
        "id": "Ou-JTFHbEF22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(df['word_count'], kde=True)"
      ],
      "metadata": {
        "id": "yPigsETPS58k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Count Distribution by Category**"
      ],
      "metadata": {
        "id": "XWD9AxMMENoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the figure and axes\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot KDE for category 1\n",
        "sns.kdeplot(df[df['category'] == 1]['word_count'], label='Positive', fill=True)\n",
        "\n",
        "# Plot KDE for category 0\n",
        "sns.kdeplot(df[df['category'] == 0]['word_count'], label='Neutral', fill=True)\n",
        "\n",
        "# Plot KDE for category -1\n",
        "sns.kdeplot(df[df['category'] == -1]['word_count'], label='Negative', fill=True)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Word Count Distribution by Category')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uAe4NNNQS_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of the plot**\n",
        "1. Positive comments (1) tends to have a wider spread in word count, meaning longer comments are more common in positive sentiments.\n",
        "2. Neutral comments (0) shows lower frequency and more concentrated around shorter comments.\n",
        "3. Negative comments (-1) have a distribution somewhat similar to positive comments but smaller proportion of longer comments."
      ],
      "metadata": {
        "id": "ljmBe-I0EbPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Boxplot**"
      ],
      "metadata": {
        "id": "BirvVQMIFCCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(df['word_count'])"
      ],
      "metadata": {
        "id": "t94qD5MhTE2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot for the 'wordcount' column categorized by 'category'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df, x='category', y='word_count')\n",
        "plt.title('Boxplot of Word Count by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Word Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "neMwnLS3TK_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatterplot between 'category' and 'wordcount'\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=df, x='category', y='word_count', alpha=0.5)\n",
        "plt.title('Scatterplot of Word Count by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Word Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9-h4Gf7BTV-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# median word counts among sentiments\n",
        "\n",
        "sns.barplot(df,x='category',y='word_count',estimator='median')"
      ],
      "metadata": {
        "id": "P9ZXJabWTY5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advance Preprocessing**"
      ],
      "metadata": {
        "id": "VLxVeVfiTd0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing nltk library (natural language toolkit)"
      ],
      "metadata": {
        "id": "jrnAZVQaFO5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "Yoi14w8nTbXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking for the stopwords**\\\n",
        "Creating a new column to check number of stopwords in a comment"
      ],
      "metadata": {
        "id": "CI0Cu-pSK4mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Create a new col 'num_stop_words' by counting the number of stopwords in each comment\n",
        "df['num_stop_words'] = df['clean_comment'].apply(lambda x: len([word for word in x.split() if word in stop_words]))"
      ],
      "metadata": {
        "id": "AvRgCho6Tg8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sample(5)"
      ],
      "metadata": {
        "id": "hM4uwtJQU2nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the stopwords frequency**"
      ],
      "metadata": {
        "id": "6MI9mjSjK_Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a distribution plot (displot) for the 'num_stop_words' column\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df['num_stop_words'], kde=True)\n",
        "plt.title('Distribution of Stop Word Count in Comments')\n",
        "plt.xlabel('Number of Stop Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EyWU2eTRU5kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stopwords by the category"
      ],
      "metadata": {
        "id": "SXITsYOFLZgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the figure and axes\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot KDE for category 1\n",
        "sns.kdeplot(df[df['category'] == 1]['num_stop_words'], label='Positive', fill=True)\n",
        "\n",
        "# Plot KDE for category 0\n",
        "sns.kdeplot(df[df['category'] == 0]['num_stop_words'], label='Neutral', fill=True)\n",
        "\n",
        "# Plot KDE for category -1\n",
        "sns.kdeplot(df[df['category'] == -1]['num_stop_words'], label='Negative', fill=True)\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Num stop words Distribution by Category')\n",
        "plt.xlabel('Stop word count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "# Add a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h_woDNG1U84b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# median word counts among sentiments\n",
        "\n",
        "sns.barplot(df,x='category',y='num_stop_words',estimator='median')"
      ],
      "metadata": {
        "id": "6kxJ6N53VAF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 25 stopwords by frequency"
      ],
      "metadata": {
        "id": "QSTBC3vzLf8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a frequency distribution of stop words in the 'clean_comment' column\n",
        "from collections import Counter\n",
        "\n",
        "# Extract all stop words from the comments using the previously defined 'common_stopwords'\n",
        "all_stop_words = [word for comment in df['clean_comment'] for word in comment.split() if word in stop_words]\n",
        "\n",
        "# Count the most common stop words\n",
        "most_common_stop_words = Counter(all_stop_words).most_common(25)\n",
        "\n",
        "# Convert the most common stop words to a DataFrame for plotting\n",
        "top_25_df = pd.DataFrame(most_common_stop_words, columns=['stop_word', 'count'])\n",
        "\n",
        "# Create the barplot for the top 25 most common stop words\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=top_25_df, x='count', y='stop_word', palette='viridis')\n",
        "plt.title('Top 25 Most Common Stop Words')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Stop Word')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JpH9vDdqVT7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of characters in a comment to compare the ratio of stopwords with the total"
      ],
      "metadata": {
        "id": "RiEMOrVPLlhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_chars'] = df['clean_comment'].apply(len)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "XxXeBc5ZVkYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_chars'].describe()"
      ],
      "metadata": {
        "id": "WEfQ4vbCVnSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of special characters in comments**"
      ],
      "metadata": {
        "id": "yh2R5qVpLxYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Combine all comments into one large string\n",
        "all_text = ' '.join(df['clean_comment'])\n",
        "\n",
        "# Count the frequency of each character\n",
        "char_frequency = Counter(all_text)\n",
        "\n",
        "# Convert the character frequency into a DataFrame for better display\n",
        "char_frequency_df = pd.DataFrame(char_frequency.items(), columns=['character', 'frequency']).sort_values(by='frequency', ascending=False)\n"
      ],
      "metadata": {
        "id": "Ojp76pY8VpvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_frequency_df['character'].values"
      ],
      "metadata": {
        "id": "FEln-ULHVsBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_frequency_df.tail(50)"
      ],
      "metadata": {
        "id": "cE3oGNJcVw1Q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Number of punctuation characters in a comment**"
      ],
      "metadata": {
        "id": "yCWiJu3wL8UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'num_punctuation_chars' to count punctuation characters in each comment\n",
        "df['num_punctuation_chars'] = df['clean_comment'].apply(\n",
        "    lambda x: sum([1 for char in x if char in '.,!?;:\"\\'()[]{}-'])\n",
        ")\n",
        "\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "qRs9xWgyV0Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['num_punctuation_chars'].describe()"
      ],
      "metadata": {
        "id": "Tso6n1bmWCaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs a textual EDA by identifying and visualizing the top 25 most frequent pairs of consecutive words (bigrams) in a dataset. It uses CountVectorizer to filter out stop words and calculate frequencies, then renders a bar chart to highlight common themes or patterns in the text."
      ],
      "metadata": {
        "id": "VeWL-aVFMGLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a function to extract the top 25 bigrams\n",
        "def get_top_ngrams(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Get the top 25 bigrams\n",
        "top_25_bigrams = get_top_ngrams(df['clean_comment'], 25)\n",
        "\n",
        "# Convert the bigrams into a DataFrame for plotting\n",
        "top_25_bigrams_df = pd.DataFrame(top_25_bigrams, columns=['bigram', 'count'])\n",
        "\n",
        "# Plot the countplot for the top 25 bigrams\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=top_25_bigrams_df, x='count', y='bigram', palette='magma')\n",
        "plt.title('Top 25 Most Common Bigrams')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Bigram')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wdVHFXgOWEvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code expands our analysis to trigrams, capturing three-word sequences to provide deeper context and identify recurring phrases that bigrams might miss. By adjusting the ngram_range to (3, 3), it isolates specific linguistic patterns, helping us see more complex themes within the clean_comment data."
      ],
      "metadata": {
        "id": "s8ORo7BVMhhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to extract the top 25 trigrams\n",
        "def get_top_trigrams(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]\n",
        "\n",
        "# Get the top 25 trigrams\n",
        "top_25_trigrams = get_top_trigrams(df['clean_comment'], 25)\n",
        "\n",
        "# Convert the trigrams into a DataFrame for plotting\n",
        "top_25_trigrams_df = pd.DataFrame(top_25_trigrams, columns=['trigram', 'count'])\n",
        "\n",
        "# Plot the countplot for the top 25 trigrams\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=top_25_trigrams_df, x='count', y='trigram', palette='coolwarm')\n",
        "plt.title('Top 25 Most Common Trigrams')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Trigram')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OJC4zEnqWG4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing all the non-English character from the comment column**"
      ],
      "metadata": {
        "id": "JPUIu7GmMt2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove non-English characters from the 'clean_comment' column\n",
        "# Keeping only standard English letters, digits, and common punctuation\n",
        "import re\n",
        "\n",
        "df['clean_comment'] = df['clean_comment'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s!?.,]', '', str(x)))"
      ],
      "metadata": {
        "id": "wmT-Bg7IWL4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_text = ' '.join(df['clean_comment'])\n",
        "\n",
        "# Count the frequency of each character\n",
        "char_frequency = Counter(all_text)\n",
        "\n",
        "# Convert the character frequency into a DataFrame for better display\n",
        "char_frequency_df = pd.DataFrame(char_frequency.items(), columns=['character', 'frequency']).sort_values(by='frequency', ascending=False)\n",
        "\n",
        "char_frequency_df"
      ],
      "metadata": {
        "id": "2ldp7ZOjWQsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qB9sAwAIWSja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keeping some of the essential stopwords**\\\n",
        " To analysis the sentiments, some stopwords like 'yes','no','but' are important. It helps us to differentiate between \"I like this movie\" and \"I do not like this movie\""
      ],
      "metadata": {
        "id": "oxrF9uabM64F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Defining stop words but keeping essential ones for sentiment analysis\n",
        "stop_words = set(stopwords.words('english')) - {'not', 'but', 'however', 'no', 'yet'}\n",
        "\n",
        "# Remove stop words from 'clean_comment' column, retaining essential ones\n",
        "df['clean_comment'] = df['clean_comment'].apply(\n",
        "    lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words])\n",
        ")"
      ],
      "metadata": {
        "id": "LLlZyUD3WVNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "zQeOpVCwW59n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Lemmatizer** \\\n",
        "To bring the words to the root form"
      ],
      "metadata": {
        "id": "JXHFUiW6NZfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Define the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to the 'clean_comment_no_stopwords' column\n",
        "df['clean_comment'] = df['clean_comment'].apply(\n",
        "    lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()])\n",
        ")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lylgjsXXW8ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing WordCloud** \\\n",
        "To analysis in the bigger picture about the comments\n"
      ],
      "metadata": {
        "id": "fkX9rWF2NjrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_word_cloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "plot_word_cloud(df['clean_comment'])"
      ],
      "metadata": {
        "id": "Lp54lmlIXdt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Most frequency data**"
      ],
      "metadata": {
        "id": "Vt-ZYA3BNxwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_n_words(df, n=20):\n",
        "    \"\"\"Plot the top N most frequent words in the dataset.\"\"\"\n",
        "    # Flatten all words in the content column\n",
        "    words = ' '.join(df['clean_comment']).split()\n",
        "\n",
        "    # Get the top N most common words\n",
        "    counter = Counter(words)\n",
        "    most_common_words = counter.most_common(n)\n",
        "\n",
        "    # Split the words and their counts for plotting\n",
        "    words, counts = zip(*most_common_words)\n",
        "\n",
        "    # Plot the top N words\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(words))\n",
        "    plt.title(f'Top {n} Most Frequent Words')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "plot_top_n_words(df, n=50)"
      ],
      "metadata": {
        "id": "1y9V8dAMXjvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Most occured words with category**\\\n",
        "words that are used in different category"
      ],
      "metadata": {
        "id": "TeqsFZlVN1rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_n_words_by_category(df, n=20, start=0):\n",
        "    \"\"\"Plot the top N most frequent words in the dataset with stacked hue based on sentiment category.\"\"\"\n",
        "    # Flatten all words in the content column and count their occurrences by category\n",
        "    word_category_counts = {}\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        words = row['clean_comment'].split()\n",
        "        category = row['category']  # Assuming 'category' column exists for -1, 0, 1 labels\n",
        "\n",
        "        for word in words:\n",
        "            if word not in word_category_counts:\n",
        "                word_category_counts[word] = { -1: 0, 0: 0, 1: 0 }  # Initialize counts for each sentiment category\n",
        "\n",
        "            # Increment the count for the corresponding sentiment category\n",
        "            word_category_counts[word][category] += 1\n",
        "\n",
        "    # Get total counts across all categories for each word\n",
        "    total_word_counts = {word: sum(counts.values()) for word, counts in word_category_counts.items()}\n",
        "\n",
        "    # Get the top N most frequent words across all categories\n",
        "    most_common_words = sorted(total_word_counts.items(), key=lambda x: x[1], reverse=True)[start:start+n]\n",
        "    top_words = [word for word, _ in most_common_words]\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    word_labels = top_words\n",
        "    negative_counts = [word_category_counts[word][-1] for word in top_words]\n",
        "    neutral_counts = [word_category_counts[word][0] for word in top_words]\n",
        "    positive_counts = [word_category_counts[word][1] for word in top_words]\n",
        "\n",
        "    # Plot the stacked bar chart\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    bar_width = 0.75\n",
        "\n",
        "    # Plot negative, neutral, and positive counts in a stacked manner\n",
        "    plt.barh(word_labels, negative_counts, color='red', label='Negative (-1)', height=bar_width)\n",
        "    plt.barh(word_labels, neutral_counts, left=negative_counts, color='gray', label='Neutral (0)', height=bar_width)\n",
        "    plt.barh(word_labels, positive_counts, left=[i+j for i,j in zip(negative_counts, neutral_counts)], color='green', label='Positive (1)', height=bar_width)\n",
        "\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Words')\n",
        "    plt.title(f'Top {n} Most Frequent Words with Stacked Sentiment Categories')\n",
        "    plt.legend(title='Sentiment', loc='lower right')\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to show the highest frequency at the top\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plot_top_n_words_by_category(df, n=20)"
      ],
      "metadata": {
        "id": "Mk7QOuuZXpMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0GPgCF5XqZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}